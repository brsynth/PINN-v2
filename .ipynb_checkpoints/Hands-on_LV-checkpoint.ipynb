{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02bdcde",
   "metadata": {},
   "source": [
    "# Hands-on : PINN for Lotka-Volterra equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df158e7",
   "metadata": {},
   "source": [
    "#### First part : imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ee1949",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from tools import random_ranges\n",
    "import numpy as np\n",
    "from lib.pinn_auxloss_f import Pinn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from torch import nn\n",
    "\n",
    "#Seed specification\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7227ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating a dataset\n",
    "def acetate_overflow_model(\n",
    "    t,\n",
    "    alp=0.8,\n",
    "    bet=0.4,\n",
    "    delt=0.2,\n",
    "    gam=0.6,\n",
    "):\n",
    "    def func(y, t):\n",
    "\n",
    "        X,Y = [y[i] for i in range(len(y))]\n",
    "\n",
    "        dXdt = X*(alp - bet*Y)\n",
    "\n",
    "        dYdt = Y*(delt*X - gam)\n",
    "\n",
    "        return np.array([dXdt, dYdt])\n",
    "\n",
    "    y0 = [5., 3.] #Conditions initiales\n",
    "    return odeint(func, y0, t)\n",
    "\n",
    "t = np.linspace(0,10,30)\n",
    "y = acetate_overflow_model(np.ravel(t))\n",
    "\n",
    "##Adding noise in the dataset\n",
    "def add_random_noise(array: np.ndarray, seed: int = None, noise_scale: float = 0.5) -> np.ndarray:\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    noise = np.random.rand(*array.shape)  # random floats in [0, 1)\n",
    "    signs = np.random.choice([-1, 1], size=array.shape)  # random +/- 1\n",
    "    return array + signs * noise * noise_scale\n",
    "data=add_random_noise(y)\n",
    "\n",
    "#Creating the auxiliary data â€“ for T_init and T_f\n",
    "data_aux=[torch.tensor([5.,2.]),\n",
    "              torch.tensor(y[-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59362645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dictionaries\n",
    "\n",
    "## Dictionary with the true value of parameters\n",
    "ode_parameters_dict = {\"alp\":0.8,\n",
    "                       \"bet\":0.4,\n",
    "                       \"delt\":0.2,\n",
    "                       \"gam\":0.6\n",
    "                       }\n",
    "## Dictionary with the parameter ranges\n",
    "ode_parameter_ranges_dict = {\"alp\":(0.,1.),\n",
    "                             \"bet\":(0.,1.),\n",
    "                             \"delt\":(0.,1.),\n",
    "                             \"gam\":(0.,1.)\n",
    "                            }\n",
    "## Dictionary with the std of variables\n",
    "std_per_variable = np.std(data, axis=0)\n",
    "variable_standard_deviations_dict = {\"X\":std_per_variable[0],\n",
    "                                     \"Y\":std_per_variable[1]\n",
    "                                    }\n",
    "\n",
    "##Dictionary of residuals\n",
    "ODE_residuals = {\"ode_1\" : \n",
    "                 lambda var_dict,d_dt_var_dict,value,min_var_dict,max_var_dict : \n",
    "                     d_dt_var_dict[\"X\"] - var_dict[\"X\"]*(value[\"alp\"] - value[\"bet\"]*var_dict[\"Y\"]),\n",
    "                 \"ode_2\" : \n",
    "                 lambda var_dict,d_dt_var_dict,value,min_var_dict,max_var_dict : \n",
    "                     d_dt_var_dict[\"Y\"] - var_dict[\"Y\"]*(value[\"delt\"]*var_dict[\"X\"] - value[\"gam\"])\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63288f",
   "metadata": {},
   "source": [
    "#### Defining the inputs of the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Names of the variables associated with data\n",
    "observables = [\"X\",\"Y\"] \n",
    "\n",
    "##Dictionaries for the data and no-data variables (and associated data)\n",
    "variable_data = {\"X\": data[:,0], \"Y\": data[:,1]} \n",
    "variable_no_data  = {}\n",
    "\n",
    "##Time points specification\n",
    "data_t = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f42b80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_ranges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgam\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Specifying the ranges from the dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m ranges \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_ranges\u001b[49m([ode_parameters_dict[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m parameter_names],scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(parameter_names):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m ode_parameter_ranges_dict:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_ranges' is not defined"
     ]
    }
   ],
   "source": [
    "#List of the names of unknown parameters\n",
    "parameter_names = [\"alp\",\n",
    "                   \"bet\",\n",
    "                   \"delt\",\n",
    "                   \"gam\"]\n",
    "\n",
    "#Specifying the ranges from the dictionary\n",
    "ranges = random_ranges([ode_parameters_dict[key] for key in parameter_names],scale=20)\n",
    "for i,name in enumerate(parameter_names):\n",
    "    if name in ode_parameter_ranges_dict:\n",
    "        ranges[i]= ode_parameter_ranges_dict[name]\n",
    "        \n",
    "#Specifying the constants, i.e. the true values of parameters\n",
    "constants_dict = ode_parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e91e88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epoch_number = 150000\n",
    "\n",
    "# Optimizer parameters\n",
    "optimizer_type = \"Adam\"\n",
    "optimizer_hyperparameters = {\"lr\":1e-4, \"betas\":(0.9, 0.8)}\n",
    "\n",
    "# Scheduler parameters\n",
    "scheduler_hyperparameters = {\"base_lr\":1e-4,\n",
    "                             \"max_lr\":1e-4,\n",
    "                             \"step_size_up\":100,\n",
    "                             \"scale_mode\":\"exp_range\",\n",
    "                             \"gamma\":0.999,\n",
    "                             \"cycle_momentum\":False}\n",
    "\n",
    "#Specifying the guiding weighting of the PINN (i.e. the weights that will multiply the balancing method output)\n",
    "residual_weights=[1,1]\n",
    "\n",
    "# Loss balancing method\n",
    "multiple_loss_method = \"prior_losses\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125aa34c",
   "metadata": {},
   "source": [
    "#### Creating and training the PINN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa358336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PINN\n",
    "pinn_cell = Pinn(ode_residual_dict=ODE_residuals,\n",
    "                 ranges=ranges,\n",
    "                 data_t=data_t,\n",
    "                 variables_data=variable_data,\n",
    "                 variables_no_data=variable_no_data,\n",
    "                 data_aux=data_aux_1mM,\n",
    "                 parameter_names=parameter_names,\n",
    "                 optimizer_type=optimizer_type,\n",
    "                 optimizer_hyperparameters=optimizer_hyperparameters,\n",
    "                 scheduler_hyperparameters=scheduler_hyperparameters,\n",
    "                 constants_dict=constants_dict,\n",
    "                 \n",
    "                 #Loss balancing specification\n",
    "                 multi_loss_method=multiple_loss_method,\n",
    "                 residual_weights=residual_weights,\n",
    "                 variable_fit_weights=None,\n",
    "                 auxiliary_fit_weights=None,\n",
    "                 \n",
    "                 #SoftAdapt parameters\n",
    "                 soft_adapt_beta=0.1,\n",
    "                 soft_adapt_t=1,\n",
    "                 soft_adapt_normalize=True,\n",
    "                 soft_adapt_by_type=True,\n",
    "                 soft_adapt_eps=10E-8,\n",
    "                 soft_adapt_warming=-1,\n",
    "                 \n",
    "                 #Increments parametrisation\n",
    "                 incr_residual_weight=20000,\n",
    "                 increment=1E2,\n",
    "                 \n",
    "                 #Prior_losses parameters\n",
    "                 prior_losses_t=100,\n",
    "                 \n",
    "                 #Wang parameters\n",
    "                 wang_residual = True,\n",
    "                 wang_t=1,\n",
    "                 wang_alpha=0.9,\n",
    "                 wang_epsilon=1E-8,\n",
    "                 wang_warming=-1,\n",
    "                 \n",
    "                 #NN parameters\n",
    "                 net_hidden=7,\n",
    "                 activation_function=nn.Softplus(),\n",
    "                 optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "r2_score, pred_variables, losses, variable_fit_losses, residual_losses, aux_losses, all_learned_parameters, learning_rates = pinn_cell.train(epoch_number) #epoch_number\n",
    "X,Y = pred_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f107b7",
   "metadata": {},
   "source": [
    "#### First outputs : loss, hyperparameters, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43828f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and Plot learning rate\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(learning_rates[0:], color = 'teal',linewidth=4)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epochs',fontsize=15)\n",
    "plt.ylabel('Learning rate',fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#To save the figure\n",
    "fig_name = 'learning_rate'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56d8229b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Print and Plot Losses\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.5g\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[43mlosses\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(losses[\u001b[38;5;241m0\u001b[39m:], color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteal\u001b[39m\u001b[38;5;124m'\u001b[39m,linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "## Print and Plot Losses\n",
    "print(\"Loss: \",\"%.5g\" % losses[-1])\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses[0:], color = 'teal',linewidth=4)\n",
    "axs[0].grid(True)\n",
    "axs[0].set_xlabel('Epochs',fontsize=15)\n",
    "axs[0].set_ylabel('Loss',fontsize=15)\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].plot(variable_fit_losses[0:], color = 'teal',linewidth=4)\n",
    "axs[1].grid(True)\n",
    "axs[1].set_xlabel('Epochs',fontsize=15)\n",
    "axs[1].set_ylabel('Variable fit loss',fontsize=15)\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "axs[2].plot(residual_losses[0:], color = 'teal',linewidth=4)\n",
    "axs[2].grid(True)\n",
    "axs[2].set_xlabel('Epochs',fontsize=15)\n",
    "axs[2].set_ylabel('Residual loss',fontsize=15)\n",
    "axs[2].set_xscale('log')\n",
    "axs[2].set_yscale('log')\n",
    "\n",
    "\n",
    "axs[3].plot(aux_losses[0:], color = 'teal',linewidth=4)\n",
    "axs[3].grid(True)\n",
    "axs[3].set_xlabel('Epochs',fontsize=15)\n",
    "axs[3].set_ylabel('Auxiliary loss',fontsize=15)\n",
    "axs[3].set_xscale('log')\n",
    "axs[3].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#To save the figure\n",
    "fig_name = 'loss_short'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496add7",
   "metadata": {},
   "source": [
    "#### Outputs : predictions and performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944adfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print and Plot R2 during optimisation \n",
    "print(\"r2 :\",\"%.5g\" % r2_score[-1])\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(r2_score, color = 'black',linewidth=4)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epochs',fontsize=15)\n",
    "plt.ylabel('Params error',fontsize=15)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#To save the figure\n",
    "fig_name = 'error_short'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a53872",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing parameters\n",
    "learned_parameters=[pinn_cell.output_param_range(v,i).item() for (i,(k,v)) in enumerate(pinn_cell.ode_parameters.items())]\n",
    "true_parameters=[ode_parameters_dict[key] for key in parameter_names]\n",
    "\n",
    "plt.grid('true')\n",
    "plt.plot([-1, 3], [-1, 3],color='black')\n",
    "plt.scatter(true_parameters,learned_parameters)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "\n",
    "cmap = plt.cm.get_cmap('viridis', len(ranges))  # Get a colormap with as many colors as there are ranges\n",
    "\n",
    "# Map each range index to a color from the colormap\n",
    "colors = [cmap(i) for i in range(len(ranges))]\n",
    "\n",
    "\n",
    "for i, (true_val, learned_val) in enumerate(zip(true_parameters, learned_parameters)):\n",
    "    plt.scatter(true_val, learned_val, s=70, color=colors[i], label=f'Range {i}' if i == 0 else \"\",zorder=3)\n",
    "\n",
    "    # Also color the corresponding vertical line\n",
    "    plt.vlines(x=true_val, ymin=ranges[i][0], ymax=ranges[i][1], colors=colors[i],zorder=2,linewidth=3)\n",
    "\n",
    "\n",
    "min_value = min(r[0] for r in ranges)\n",
    "max_value = max(r[1] for r in ranges)\n",
    "plt.ylim([-1,3])\n",
    "plt.xlim([-1,3])\n",
    "\n",
    "plt.xlabel('True parameters',fontsize=20)\n",
    "plt.ylabel('Learned parameters',fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#To save the figure\n",
    "fig_name = 'params_short'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Percentage error on parameters\n",
    "err=np.array([(abs(true_parameters[i]-learned_parameters[i])/true_parameters[i])*100 for i in range(len(true_parameters))])\n",
    "print(\"percentage error\", np.mean(err),\"\\n\")\n",
    "\n",
    "## AIC\n",
    "AIC = 2*(20+7*20+20)+2*(losses[-1]) #Formula to adjust on the number of parameters\n",
    "print(\"AIC\",AIC,\"\\n\")\n",
    "\n",
    "## R2_scores\n",
    "from sklearn.metrics import r2_score\n",
    "R2_scores_train_data=[r2_score(data[:,0],X.detach().numpy()),r2_score(data[:,1],Y.detach().numpy())]\n",
    "print(\"R2_scores_train_data\",R2_scores_train_data,\"\\n\")\n",
    "\n",
    "R2_scores_true_data=[r2_score(y[:,0],X.detach().numpy()),r2_score(y[:,1],Y.detach().numpy())]\n",
    "print(\"R2_scores_gene_data\",R2_scores_gene_data,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b79d5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Plot the predicted variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m axs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(t, y[\u001b[38;5;241m0\u001b[39m], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_true\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m axs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(data_t, X\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_pred\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "## Plot the predicted variables\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(t, y[:,0], linestyle='dashed', label='X_true', color='b')\n",
    "axs[0].plot(data_t, X.detach().numpy(), label='X_pred', color='b')\n",
    "axs[0].plot(data_t, data[:,0], 'o', label='X_data', color='b')\n",
    "axs[0].set_title('X')\n",
    "axs[0].set_xlabel('t(h)')\n",
    "axs[0].set_ylabel('X')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(t, y[:,1], linestyle='dashed', label='Y_true', color='g')\n",
    "axs[1].plot(data_t,Y.detach().numpy(), label='Y_pred', color='g')\n",
    "axs[1].plot(data_t, data[:,1], 'o', label='Y_data', color='g')\n",
    "axs[1].set_title('Y')\n",
    "axs[1].set_xlabel('t (h)')\n",
    "axs[1].set_ylabel('Y')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_name = 'prediction_plot'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrate with guessed parameters\n",
    "\n",
    "learned = ode_parameters_dict.copy()\n",
    "learned = learned | dict(zip(parameter_names, learned_parameters))\n",
    "alp, bet, delt, gam = learned.values()\n",
    "gene_data=acetate_overflow_model(np.ravel(t), alp, bet, delt, gam)\n",
    "X_learned,Y_learned= gene_data[:,0],gene_data[:,1]\n",
    "\n",
    "from tools import ssr_error\n",
    "\n",
    "# Plot the solved variables\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(t, y[:,0], linestyle='dashed', label='X_true', color='b')\n",
    "axs[0].plot(t, X_learned, label='X_learned', color='b')\n",
    "axs[0].plot(data_t, data[:,0], 'o', label='X_data', color='b')\n",
    "axs[0].set_title('X')\n",
    "axs[0].set_xlabel('t (h)')\n",
    "axs[0].set_ylabel('X')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(t, y[:,1], linestyle='dashed', label='Y_true', color='g')\n",
    "axs[1].plot(t, Y_learned, label='Y_learned', color='g')\n",
    "axs[1].plot(data_t, data[:,1], 'o', label='Y_data', color='g')\n",
    "axs[1].set_title('Y')\n",
    "axs[1].set_xlabel('t (h)')\n",
    "axs[1].set_ylabel('Y')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#To save the figure\n",
    "fig_name = 'predicted_variables'\n",
    "plt.savefig(fig_name+'.png', format='png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Values of parameters\n",
    "variable_res = {\"X\":X_learned,\n",
    "                \"Y\":Y_learned}\n",
    "\n",
    "error = ssr_error(standard_deviations_dict=variable_standard_deviations_dict, observables=observables, variable_data=variable_data, variable_res=variable_res)\n",
    "print(\"Sum of squared residuals error: \" + str(error)+\" \\n\")\n",
    "\n",
    "## R2 scores\n",
    "R2_scores_vs_data=[r2_score(y[:,0],X_learned),r2_score(y[:,1],Y_learned)]\n",
    "print(\"R2_scores_vs_data\",R2_scores_vs_data,\"\\n\")\n",
    "\n",
    "R2_scores_gene_vs_learned=[r2_score(data[:,0],X_learned),r2_score(data[:,1],Y_learned)]\n",
    "print(\"R2_scores_gene_vs_learned\",R2_scores_gene_vs_learned,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bac739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and compare parameters\n",
    "from tools import param_error_percentages #lib.\n",
    "\n",
    "print(\"Learned parameters :\")\n",
    "for i in range(len(learned_parameters)):\n",
    "    print(parameter_names[i], \":\", learned_parameters[i])\n",
    "\n",
    "print(\"\\nTrue parameters :\")\n",
    "for i in range(len(learned_parameters)):\n",
    "    print(parameter_names[i], \":\", true_parameters[i])\n",
    "\n",
    "print(\"\\nParameters errors :\")\n",
    "for i in range(len(learned_parameters)):\n",
    "    print(parameter_names[i], \":\", param_error_percentages(true_parameters,learned_parameters)[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
